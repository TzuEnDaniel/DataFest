library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(quanteda.textmodels)
library(dplyr)
library(data.table)
library(ggplot2)
library(tidyverse)

datafest <- read.csv("questionposts.csv")
cateData <- read.csv('questions.csv')
names(datafest)

# subset the data

pa_text <- filter(datafest, StateAbbr == "PA")
tibble_cat<-as_tibble(cateData)
tibble_cat<-tibble_cat%>%
  separate(Date,into = c('Year','Month','Day'),sep='-')
tibble_catDate<-tibble_cat%>%
  unite(NewDate,Year,Month,sep='-')
GroupCate<-tibble_catDate%>%
  group_by(NewDate,Category)%>%
  summarise(count=n())
GroupYear<-tibble_cat%>%
  group_by(Year,Category)%>%
  summarise(count=n())
GroupStateYear<-tibble_cat%>%
  filter(Year=='2021')%>%
  group_by(Month,StateAbbr)%>%
  filter(StateAbbr!='PA')%>%
  summarise(count=n())
StateCount<-GroupStateYear%>%group_by(StateAbbr)%>%
  summarise(count=n())
str(GroupCate)
GroupYear<-read.csv('GroupYear.csv')
GroupYearDesc<-GroupYear%>%
  arrange(Year,desc(count))
GroupCate$NewDate<-as.character(GroupCate)
GroupCate<-as.data.frame(GroupCate)
GroupStateYear<-GroupStateYear%>%
  arrange(Month,desc(count))
write.csv(GroupStateYear,".\\GroupStateYear.csv", row.names=FALSE)
df_cat<-as.data.frame(tibble_cat)
str(df_cat)
df_cat$Month<-factor(df_cat$Month)
ggplot(df_cat,
       aes(x =Month )) +
  geom_histogram()
df_catCountPerMonth<-df_cat%>%
  group_by(Month,Category)%>%
  summarise(count=n())
df_catCountPerState<-df_cat%>%
  group_by(StateAbbr,Category)%>%
  summarise(count=n())
ggplot(df_cat, aes(x = Month, fill  = Category)) + 
  geom_bar(stat='count') + 
  ggtitle("Month Frequency Plot")
ggplot(df_cat, aes(x = StateAbbr, fill  = Category)) + 
  geom_bar(stat='count') + 
  ggtitle("Frequency Plot Per State")
GroupYear %>%
  ggplot( aes(x=Year, y=count, group=Category, color=Category)) +
  geom_line()+
  ggtitle('Category line chart')
df_catCountPerMonth %>%
  ggplot( aes(x=Year, y=count, group=Category, color=Category)) +
  geom_line()+
  ggtitle('Category line chart')
df_cat
# prepare a corpus

pa_corpus <- corpus(pa_text, text_field = "PostText")
all_cat<-corpus(cateData,text_field = 'Category')
# the spreadsheet columns are still associated with the texts
# in docvars

head(docvars(pa_corpus))

# so you can select things based on the other parts of the dataset
# I'm using the %like% function from the data.table package to 
# find a string, but you will probably have other ways of doing this

summary(corpus_subset(pa_corpus, CreatedUtc %like% "2021-07-10"))
jul10 <- corpus_subset(pa_corpus, CreatedUtc %like% "2021-07-10")

# collocations (uses the corpus, but the corpus needs some cleaning)

pa_corpClean <- tokens(pa_corpus, remove_punct = TRUE, remove_symbols = TRUE)

# size is the number of collocated words; results in a dataframe

pa_coll <- textstat_collocations(pa_corpClean, size=2, min_count = 25, tolower = TRUE)

# keywords or phrases in context, generates a dataframe
# window is how many words you want to see on both sides of the keyword

pa_custody <- kwic(pa_corpClean, pattern = "custody", window = 5)

pa_courtOrder <- kwic(pa_corpClean, phrase("court order"), window = 5)

# create a document-feature matrix (not a dataframe)

pa_dfm <- tokens(pa_corpus, remove_symbols = TRUE, remove_punct = TRUE) %>%
  tokens_remove(stopwords("en")) %>%
  dfm()
cat_dfm <- tokens(all_cat, remove_symbols = TRUE, remove_punct = TRUE) %>%
  tokens_remove(stopwords("en")) %>%
  dfm()
# what does this DFM look like?

View(pa_dfm)

# making use of docvars; these two do the same thing

pa_dfm$Id
docvars(pa_dfm, field = "Id")

# can subset after creating a DFM
# as I did above, for example:

dfm_subset(pa_dfm, StateAbbr == "PA")
dfm_subset(pa_dfm, Id < 270599)
dfm_subset(pa_dfm, CreatedUtc %like% "2021-07-10")

# If you generate your dfm from the full dataset
# the dfm_group function is another option if you want to summarize by,
# for example, state
# not going to run the below, but it gives you the general idea

dfm_state <- dfm_group(pa_dfm, groups = StateAbbr)

# word frequencies

topfeatures(pa_dfm, 50)

textplot_wordcloud(cat_dfm, min_count = 50, 
                   color = RColorBrewer::brewer.pal(8, "Dark2"), 
                   labelsize = 5,)

# Can always do additional cleanup with dfm_remove function

pa_dfm <- dfm_remove(pa_dfm, pattern = c("s", "t"))

# Plot just the most frequent words using the dataframe generated by textstat_frequency
# Can also see how many documents contain the word in the docfreq col

features_dfm <- textstat_frequency(cat_dfm, n = 50)

# Sort by reverse frequency order
features_dfm$feature <- with(features_dfm, reorder(feature, -frequency))

ggplot(features_dfm, aes(x = feature, y = frequency)) +
  geom_point() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# or

ggplot(features_dfm[1:20, ], aes(x = reorder(feature, docfreq, y = docfreq))) +
  geom_point() +
  coord_flip() +
  labs(x = NULL, y = "docfreq")
